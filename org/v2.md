telehash v2 (draft)
===================

# Introduction

(note: this is the second major version of the telehash protocol, the first one is deprecated and was a minimal experimental spec to create a distributed hash table, it is a work in progress yet and unstable!)

Telehash is a new encrypted mesh protocol enabling applications to find, identify, and communicate directly with each other.  It is built on public-key security (PKI) and fundamentally creates peer-to-peer (P2P) connections using a distributed hash-table (DHT) to form the mesh.  As a protocol it doesn't provide direct end-user functionality (you won't find "client apps" here) but is primarily a tool for developers (specs and libraries/SDKs) to use in creating modern apps that require rich experiences and private interconnectivity at scale.

The principle idea that drove the creation and development of telehash is the belief that any app should be able to easily and securely talk to any other app (either two instances of the same app or between completely different apps) directly and in any environment (servers, mobile, sensors, etc).  By enabling this freedom for developers as a foundation for their apps it then enables the same freedom for the people using them, that they can connect, share, and communicate privately more easily.

The challenges and complexity right now in connecting apps via existing technologies such as APIs, OAuth, and REST is only increasing, often forcing fundamentally insecure centralized and closed/gated communication platforms.  By adopting telehash in any app it immediately has a powerful set of open tools for not only its own needs, but can then also enable connectivity to and from apps created by others easily. These tools include the ability to have friends, sharing, feeds, tagging, search, notifications, discovery, and other social patterns.

The foundation of the protocol builds on JSON as the core extensible data format with a requirement to keep it simple and lightweight enough to support apps running on networked devices and sensors. The design goals include not forcing any particular architecture design such as client-server, centralized/federated/distributed, polling/push, REST, streaming, publish-subscribe, or message passing... any can be used as telehash simply facilitates creating the bi-directional connectivity between any two or more apps.

There's some high level concepts that are important to understand when talking about anything using telehash, and the first one is that of a "network", the term describing how all telehash apps are organized.  A network is identified by it's hostname (such as "telehash.org" is one network) and is the primary form of trust and connectivity between apps.  Each network is it's own DHT and has one or more "operators" which facilitate access to other apps in the network (act as a seed), and can provide other administrative services depending on the app.

Every instance of an app has a unique public id on each network it's connected to that is called it's "hashname" and is the primary means of finding and communicating with other instances in the network. Any app may have one or more networks that it's connected to, including private ones for app-specific services and public ones that are providing functionality to many apps.

# Getting Started

In order to use telehash in an app it will need to include a switch, the software layer that talks to the network and processes packets.  Each unique instances of an app must also generate or have it's own private RSA key-pair, which is used to used to create it's hashname(s) and encrypt the data sent on any network.  Once it's connected it can find and access other hashnames as well as provide services to back out to anyone on that network.  

Most apps will need their own network for access to their back-end, user management, profiles, notifications, etc facilities. To create a new network you simply need to add a DNS SRV record to the hostname for your network that identifies one or more hashnames of operators that manage it.  (todo show example operators, software and patterns)

## Telehash Switches

The software implementations of the telehash protocol are called a "switch" and it's highly suggested to use an existing switch library or service for your platform or language versus trying to create one from scratch in order to ensure that the security and identity aspects are verified properly. If there isn't one yet then we'd love your help, pull requests to list them here are welcome!

* Node.js - [node-telehash](http://github.com/quartzjer/node-telehash)

---

# Protocol Details

## Vocab

* DHT - Distributed Hash Table (based on [Kademlia](http://en.wikipedia.org/wiki/Kademlia))
* NAT - A device/router that acts as a bridge to internal IPPs (Network Address Translation)
* Hashname - The unique ID of an individual application/instance using telehash
* Packet - A UDP packet less than 1400 bytes sent between any hashnames
* Switch - The name of the software layer or service parsing packets for one or more hashnames
* Network - A unique name representing a collection of hashnames that form a unique single DHT
* Line - When any two hashnames connect and exchange their identity to form a temporary encrypted session

### Hashnames

Every endpoint within telehash is identified by it's hashname, a 40-character SHA1 string that is created by hashing the PEM-formatted RSA public key concatenated with the hostname of the network being used.

Example of how to create a hashname:
``` js
	var key = "-----BEGIN PUBLIC KEY-----\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEArq38dvKzL5W2QqprgQN7\nAo5OWFhX04aIrYZH5sLjOzyI0gWZ6ZzpQifRk+L1yNU3nkotKfeQF5zzZvo4F7YL\nC4fZgkCN2TnvBihKj25CHVDKLOtV01LvPvEEX+oHQyUzT90FT5UUIdOqTXHY4yT+\nnoxubQOAMSOsJHulpIMeDR+hPWYuZ5eZWfRimu0vEE1ujAeKGUk5avKtNtJIRDXB\nRRem/CBLPG5QRe+54w94Xwp1l3VQdJaD+qRKBEG/hhSVqUHfRqVccNR4AV+q37XG\nDAupGc7YUJ6Pqnj7TnapQGrSno13IG+2PIhL3gB1lMWEGE/hwN1dxuUAXXsIgPU3\nKwIDAQAB\n-----END PUBLIC KEY-----\n";
	var network = "telehash.org";
	var hashname = require("crypto").createHash("sha1").update(key + network).digest('hex');
```

Since the DHT is based on Kademlia this makes a [Sybil attack](http://en.wikipedia.org/wiki/Sybil_attack) more difficult, but not impossible, particularly for smaller networks.  Each network needs to use additional mechanisms (such as social ones assisted by the app layer) to ensure that the hashnames joining it's DHT are not being maliciously created.

### Networks

A network is simply a collection of apps that have a pre-known relationship or some trust each other and is typically an app or service's domain name, like "telehash.org".

All hashnames that are part of a network must have a seed list in order to join/create the DHT.  This list of seeds must contain at least an IP, Port, and the Public Key for each seed entry in order for the hashname to connect to them when starting.  The seed list may also change over time to include new stable or trusted hashnames in that network.

## Parsing

Every packet must begin with two bytes that are a short unsigned integer representing the length of bytes that follow it.  That length of bytes are UTF8 encoded JSON and can be parsed by any JSON parser.  The JSON is required so the length must be greater than two (the minimum JSON "{}" string) and less than the length of the raw UDP message minus two (the bytes for the short unsigned integer). Any remaining bytes on the packet are considered raw binary and referenced as the 'BODY' when used in definitions below.

    <length><JSON>[BODY]

Example decoding logic in node (simplified):
``` js
dgram.createSocket("udp4", function(msg){
	var length = msg.readUInt16BE(0);
    var js = JSON.parse(msg.toString("utf8", 2, length + 2));
	var body = msg.slice(length + 2);
});
```

It is a very common pattern for multiple packets to be included in one UDP message recursively, where the BODY is actually another packet, so switch implementations must be prepared to decode a packet generically on demand from a set of raw bytes.

### JSON

The JSON often acts as the header for a packet that has a binary payload, or can be the entire thing.  The fields used vary depending on the context and are specified below, but all initial packets contain a "type" field with a string value.


### BODY

The optional BODY is always a raw binary of the remainder bytes between the packet's total length and that of the JSON as indicated by LENGTH above. Often a BODY is another full raw packet and will be decoded identically to being read from the UDP socket, this pattern of one packet enclosing/attaching another is how the RSA signatures and encryption are implemented.

The BODY is also used as the raw (optionally binary) content transport for streams and for any app-specific usage.

# Packet Processing

When a packet is being processed from a UDP message initially, it's JSON must contain a "type" field with a string value of "open" or "line", each of which determine how to process the BODY of the packet:

* [open](#open) - this is a request to establish a new encrypted session (a `line`) and contains the data to do so
* [line](#line) - this is an encrypted packet for an already established session

Once a line is established, all packets sent thereafter within a line will contain a [stream](#stream) as the content-transport method between any two hashnames.

<a name="open" />
## `open` - Establishing a Line

When a packet is of `"type":"open"` it must also contain an `"open":"TYtmBdke6QWx..."` and `"sig":"mj2vNXh0b45a..."` along with it.  These are used to establish a temporary session between any two hashnames, called a "line".

The string value of the "open" key is created by generating a random secret, and encrypting it *to* the recipients public key, using RSA PKCS1 padding and base64 encoding.  The recipient can then decrypt this using their private key and get the contained secret.  This secret is used to decipher the included binary body using AES-128-CBC, and once unencrypted it is decoded and processed as another packet.

The string value of the "sig" key is an RSA HMAC-MD5 signature encoded as base64, and that is created by the sender signing the binary/encrypted body. To verify, first decrypt the body using the secret (above), and after decoding the enclosed packet, it's BODY will be the sender's public key. Using that the signature value can be validated.

Once the decoded packet's signature is verified, it MUST also contain the following three fields in the JSON:

* `to` - which hashname this line is being created to
* `line` - the unique id the recipient must use for sending any line packets, a 40 length hex value (SHA1)
* `at` - an integer timestamp of when it was sent, used to another open request is newer

<a name="line" />
## `line` - Packet Encryption

A packet with a `"type":"line"` is only sent/valid after an open has been exchanged, and is required to have a `"line":"3ec1e50ad96e1895a3ed640c908ec78546e29816"` with the value being the same as the recipient sent in it's open.  This ensures that no line packets can be received without being invited in an open. Any unknown ones are just ignored.

The BODY is a binary encoded encrypted packet using AES-128-CBC and the secret from the sender's original open packet.  Once decrypted, the recipient then processes it as a normal packet (LENGTH/JSON/BODY) from the sending hashname.  All decrypted packets must contain a `"stream":"2588f32b66ac88cb4d02a78d62d1671814b18b82"` value as defined below.

<a name="stream" />
## `stream` - Content Transport

All data sent between any two hashnames (inside a line packet) must contain a `"stream":"2588f32b66ac88cb4d02a78d62d1671814b18b82"` with a unique 40 length hex value (SHA1) determined by the sender for each different exchange or channel.  A stream always begins with a `"type":"..."` to tell the recipient what kind of stream it is, and may not have any response, or may be long-lived with many packets exchanged using the same "stream" identifier (depending on the type of stream). At any point a stream can be closed by sending an `"end":true`, and if it was an error a string message can be included as the value of the `"err":"message"`.

The following values for `type` are part of the core spec that all switches must implement:

* [seek](#seek) - return any pointers to other closer hashnames for the given `hash` (DHT), answer contains `see`
* [peer](#peer) - ask the recipient to make an introduction to one of it's peers
* [connect](#connect) - a request asking to try to open a connection to a given hashname (result of a `peer`)

Additional `type` values can be defined by any application as long as they begin with an underscore like `"type":"_custom"`.  Common patterns are also being defined as extensions, such that switches can start to support them internally easily:

* [sockets](ext_sockets.md) - raw socket proxy/transport
* [tickets](ext_tickets.md) - a way to create portable data containers that are signed by a hashname

All UDP packets are by their very nature lossy so streams also inherently supports reliability, retransmission, and buffering/backpressure mechanisms by requiring a lightweight `"seq":0` field on every packet. All seq values start at 0 and increment per packet sent. A buffer of these packets must be kept keyed by the seq value until the receiving hashname has responded confirming them in a `ack` and not in the `miss`. The `ack` is the highest known `seq` value received. The `miss` is an array of integers and must be sent along with any `ack` if in the process of receiving packets there were any missing sequences, containing in any order the missing sequence values up to the `ack`.  Upon receipt those missed packets should be resent verbatim.

By default a stream should be invalidated if a sequence has been missed three or more times, or there's more than 100 missed packets by default (senders cannot send more than that without a confirming range). When there's consistently missing packets, senders should limit the number of packets beyond the confirmed range. (needs more examples/definition)

When reliability isn't required for a stream, either side can send an `ack` of the last received `seq` value without tracking any misses, while still following the 100-max-outstanding rule to provide for any congestion/loss detection.

<a name="seek" />
### `"type":"seek"` - Finding Hashnames (DHT)

Every network is also a basic Kademlia-based DHT. The bulk of the complexity is in the rules around maintaining a mesh of lines and calculating distance explained [below](#kademlia). The `"seek":"15f4af33b7edcfbc716cd733a9ed73cc3033c17f"` value is always another hashname that the app is trying to connect to.

When one hashname wants to connect to another hashname, it finds the closest lines it knows and sends a `seek` containing the hash value to them.  They return a compact `"see":[...]` array of addresses that are closest to the hash value.  The addresses are a compound comma-delimited string containing the "hash,ip,port" (these are intentionally not JSON as the verbosity is not helpful here), for example "a9993e364706816aba3e25717850c26c9cd0d89d,123.45.67.89,10111". 

<a name="peer" />
### `"type":"peer"` - Introductions to new hashnames

For any hashname to send an open to another it must first have it's hashname, so there is a two step process starting with a peer request. Since new hashnames are discovered only from another (in the `see` values), they are tracked as a "via" so that they can be sent a peer request when a connection is being made to a hashname they sent.

This also serves as a workaround if any NAT exists, so that the two hashnames can send a packet to each other to make sure the path between them is open, this is called "hole punching." A peer request requires a `"peer":[...]` where the value is an array of hashnames the sender is trying to reach. The recipient of the peer request must then send a connect (below) to each of the target hashnames (that it already must have an open line to).

<a name="connect" />
### `"type":"connect"` - Connect to a hashname

The connect request is an immediate result of a peer request and must also contain an `"ip":"1.2.3.4"` and `"port":5678` with the values being of the peer requestor and a BODY of their public key.

The recipient can use the given IP, port, and public key to send an open request to the target.  If a NAT is suspected to exist, the target should have already sent a packet to ensure their side has a path mapped through the NAT and the open should then make it through.

# Switch Behaviors

Besides parsing the protocol, decoding the packets and processing the different stream types, in order to fully implement telehash a switch must also internally track and respond with the correct behaviors to support both the DHT for a network and manage the reliability of streams that require it.

<a name="kademlia" />
## [Kademlia](http://en.wikipedia.org/wiki/Kademlia)

(this area in progress...)

Every switch must have both a startup/seeding routine, and a background line maintenance process in order to properly support the Kademlia-based DHT for each network it's attached to.

The core data structure to support this within a switch is a list of "buckets", one for each bit of distance between it's own hashname and every other hashname encountered, 160 of them.  These buckets can contain a variable number of hashnames each, but it's recommended for any switch to limit the size of each bucket to a reasonable number more than 1 and less than 10 so as to not consume too much memory or network resources maintaining lines.

The seeding process involves recursively performing a [seek](#seek) for it's own hashname against the list of seeds (provided by the app) or operators (as resolved via DNS).

The maintenance process involves tracking all the hashnames seen (result of any seek) and trying to ensure a line is open to the minimum number of hashnames in each bucket.

A new seek should be sent to every idle/inactive line at least once every 60 seconds if the switch believes it is behind a NAT (to maintain the NAT mapping), or once every hour if it knows it is not.  By seeking it's own hashname, any new hashnames near it will be returned to help maintain the DHT.

## Stream Congestion Control

(this area in progress...)

Reliability is optional for streams based on their type, each type must define if it requires it or not, and custom ones from the application must signal if they don't require it.

